{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47a2bfeb-e844-4d7a-be12-cadf336e1d44",
   "metadata": {},
   "source": [
    "# Fetch the housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "765f656f-7fd7-4b42-bac2-c8304a29eab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "import numpy as np\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26715658-896f-436a-8676-9a1c96f1f6b0",
   "metadata": {},
   "source": [
    "# Load the housing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0ec3edb1-99cb-4aed-b3b2-316f81ebce1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfc0ee0-2fd7-41f7-98eb-535fde725f3b",
   "metadata": {},
   "source": [
    "# Splitting of data\n",
    "Fo this common method of train_test_split could be used.  But for eg, if expert say that median_income is most affecting feature, then the split should be done based on that.  Since it is not categorical, we have to bin it to proceed with this approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4eabe097-ce96-4722-a9bd-82ecbb3cef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e7c057ee-c7c0-4f36-8b27-ea4ae1e55e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "051e349e-9fab-47a1-9bc3-85a345cf40ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "03f14c45-419d-47c8-a9b8-8182fad46b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347a961f-5ae8-4a9c-9b4c-0ea1ca38722f",
   "metadata": {},
   "source": [
    "### SimpleImputer will fill missing values with median value of the feature.  It can only be done for numerical attributes, so drop ocean_proximity in this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7036b0f1-2026-4a9a-a35b-ef17c1c18a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4b7ee67e-cfd1-4759-9d61-cae3c5995da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-118.51   ,   34.26   ,   29.     , 2119.     ,  433.     ,\n",
       "       1164.     ,  408.     ,    3.54155])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "imputer.fit(housing_num)\n",
    "housing_num.median().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1658e4c1-7a85-4326-80ec-51005740e385",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(housing_num)\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
    "                          index=housing_num.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435dc6d8-6281-4d58-b7b4-0d8be2e74382",
   "metadata": {},
   "source": [
    "### One hot encoder for text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e851ad34-7bf9-4095-bc3e-1627472f348c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n",
       "       dtype=object)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "cat_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5bff84-603e-4c0f-808c-c5ad5c1a87c3",
   "metadata": {},
   "source": [
    "### Adding attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f46f2fd9-b1bd-48d7-82e2-260e2c95f50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "                         bedrooms_per_room]\n",
    "\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6053e9-50ac-4df5-9d7d-1948840983ef",
   "metadata": {},
   "source": [
    "### Make imputer, attributes addition and standardscalar into a pipeline, the last estimator must be transformers meaning they should have fit_Transform() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "79221ed3-d2d8-48c4-9fcf-d45ee8b45a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cbfc89-b5fb-4867-8fab-f7a0699b7bbb",
   "metadata": {},
   "source": [
    "## Applying transformation to all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3a2e6467-c624-40c5-a33a-1f896c03c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9986b601-d760-43a2-ad00-203ce72684cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def make_prediction_with_grid(grid_search):\n",
    "    cvres = grid_search.cv_results_\n",
    "    print(grid_search.best_params_)\n",
    "    for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "         print(np.sqrt(-mean_score), params)\n",
    "         extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "    final_model = grid_search.best_estimator_\n",
    "\n",
    "    X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "    y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "    X_test_prepared = full_pipeline.transform(X_test)\n",
    "\n",
    "    final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "    final_mse = mean_squared_error(y_test, final_predictions)\n",
    "    final_rmse = np.sqrt(final_mse)   # => evaluates to 47,730.2\n",
    "    print(final_rmse)\n",
    "    confidence = 0.95\n",
    "    squared_errors = (final_predictions - y_test) ** 2\n",
    "    print(np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n",
    "                             loc=squared_errors.mean(),\n",
    "                             scale=stats.sem(squared_errors))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24de92ec-0b6d-44e1-8c8d-77af16703334",
   "metadata": {},
   "source": [
    "### 1. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "696acadc-0284-4b2e-9b77-7103a53723d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10, 'kernel': 'linear'}\n",
      "112571.06378605746 {'C': 1, 'kernel': 'linear'}\n",
      "84649.6069847477 {'C': 10, 'kernel': 'linear'}\n",
      "118638.40200558837 {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "116126.659130923 {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "80641.57697382719\n",
      "[77898.8231993  83294.06472142]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "param_grid = [\n",
    "    {'kernel': ['linear'], 'C': [1, 10]},\n",
    "    {'kernel': ['rbf'], 'C': [1, 10], 'gamma': ['scale']}\n",
    "]\n",
    "\n",
    "svr = SVR()\n",
    "\n",
    "grid_search = GridSearchCV(svr, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "\n",
    "grid_search.fit(housing_prepared, housing_labels)\n",
    "make_prediction_with_grid(grid_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a629acb9-7dd7-442e-80d2-492d68467a9a",
   "metadata": {},
   "source": [
    "# 2. RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "79f70997-6282-4502-978d-8259e16d5e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kernel': 'rbf', 'gamma': 'scale', 'C': 1}\n",
      "118898.89058474178 {'kernel': 'rbf', 'gamma': 1, 'C': 1}\n",
      "118638.40200558837 {'kernel': 'rbf', 'gamma': 'scale', 'C': 1}\n",
      "116911.25458976308\n",
      "[113743.25976841 119995.64057398]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "param_grid = [\n",
    "    {'kernel': ['linear'], 'C': [ 1, 10]},\n",
    "    {'kernel': ['rbf'], 'C': [0.1, 1, 10], 'gamma': ['scale', 'auto', 0.01, 1]}\n",
    "]\n",
    "\n",
    "\n",
    "grid_search = RandomizedSearchCV(svr, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error', n_iter=2,\n",
    "                           return_train_score=True)\n",
    "\n",
    "grid_search.fit(housing_prepared, housing_labels)\n",
    "make_prediction_with_grid(grid_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a27507d-1e83-42d5-bf54-07f1557073e5",
   "metadata": {},
   "source": [
    "### 3. Transformer - Select K Best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "968ac96a-dca5-457c-8858-35878f56c032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112348.65576898753\n",
      "[109177.03154473 115433.16979655]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "class SelectKBestTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k\n",
    "        self.selector = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.selector = SelectKBest(score_func=f_regression, k=self.k)\n",
    "        self.selector.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.selector.transform(X)\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return self.selector.get_feature_names_out(input_features)\n",
    "\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    \n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('attribs_adder', CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "    ('feature_selection', SelectKBestTransformer(k=5)),\n",
    "])\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing, housing_labels)\n",
    "\n",
    "final_model = SVR(kernel='rbf',C=10,gamma='scale')\n",
    "\n",
    "final_model.fit(housing_prepared, housing_labels)\n",
    "\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)   # => evaluates to 47,730.2\n",
    "print(final_rmse)\n",
    "confidence = 0.95\n",
    "squared_errors = (final_predictions - y_test) ** 2\n",
    "print(np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n",
    "                               loc=squared_errors.mean(),\n",
    "                               scale=stats.sem(squared_errors))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd46eda-c332-4083-96f2-88fe9d45c3e3",
   "metadata": {},
   "source": [
    "### 4. Single Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "536b667d-d00b-4bed-8b12-1c2dd0baf284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109468.27796355786\n",
      "[106266.44648992 112579.08380786]\n"
     ]
    }
   ],
   "source": [
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('attribs_adder', CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler())]\n",
    "    )\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "col_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessor', col_pipeline),# Preprocessing steps\n",
    "    ('feature_selection', SelectKBest(f_regression, k=5)),\n",
    "    ('svr', SVR(kernel='rbf',C=10,gamma='scale'))                                        # SVR model for final prediction\n",
    "])\n",
    "\n",
    "full_pipeline.fit(housing, housing_labels)\n",
    "\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "final_predictions = full_pipeline.predict(X_test)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)   # => evaluates to 47,730.2\n",
    "print(final_rmse)\n",
    "confidence = 0.95\n",
    "squared_errors = (final_predictions - y_test) ** 2\n",
    "print(np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n",
    "                               loc=squared_errors.mean(),\n",
    "                               scale=stats.sem(squared_errors))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa0ae99-e41c-4a54-86e3-fa8a957b941e",
   "metadata": {},
   "source": [
    "### 5. Preparation options in GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "00d7b05b-97a4-49da-9098-6a80fb935bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10, 'kernel': 'linear'}\n",
      "112571.06378605746 {'C': 1, 'kernel': 'linear'}\n",
      "84649.6069847477 {'C': 10, 'kernel': 'linear'}\n",
      "118638.40200558837 {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "116126.659130923 {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "80641.57697382719\n",
      "[77898.8231993  83294.06472142]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "\n",
    "class IsolationForestTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, contamination=0.1):\n",
    "        self.contamination = contamination\n",
    "        self.model = IsolationForest(contamination=self.contamination)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.model.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Predict outliers and keep only inliers\n",
    "        inliers = self.model.predict(X) == 1\n",
    "        return X[inliers]\n",
    "param_grid = [\n",
    "    {'kernel': ['linear'], 'C': [1, 10]},\n",
    "    {'kernel': ['rbf'], 'C': [1, 10], 'gamma': ['scale']}\n",
    "]\n",
    "\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('attribs_adder', CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "    \n",
    "    ])\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "full = Pipeline([\n",
    "    ('preprocessor', full_pipeline),\n",
    "    ('outlier_detection', IsolationForest()),\n",
    "])\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "\n",
    "svr = SVR()\n",
    "\n",
    "grid_search = GridSearchCV(svr, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "\n",
    "grid_search.fit(housing_prepared, housing_labels)\n",
    "make_prediction_with_grid(grid_search)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183fa21e-cced-4ea6-9423-e2edf4a5295d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
